@Proceedings{COMPAY 20252025,
  name =     {Proceedings of the MICCAI Workshop on Computational Pathology},
  booktitle = {Proceedings of the MICCAI Workshop on Computational Pathology},
  shortname = {MICCAI COMPAYL 2025},
  year  = {2025},
  start  = {2025-9-27},
  end  = {2025-9-27},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  publisher = {PMLR},
  series =    {Proceedings of Machine Learning Research},
  volume =    {316}
}

@InProceedings{lucassen25,
  title = 	 {On the Importance of Text Preprocessing for Multimodal Representation Learning and Pathology Report Generation},
  author =       {Lucassen, Ruben T. and Luijtgaarden, Tijn van de and Moonemans, Sander P. J. and Breimer, Gerben E. and Blokx, Willeke A. M. and Veta, Mitko},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {1--11},
  year = 	 {2025},
  month = {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = {Vision-language models in pathology enable multimodal case retrieval and automated report generation. Many of the models developed so far, however, have been trained on pathology reports that include information which cannot be inferred from paired whole slide images (e.g., patient history), potentially leading to hallucinated sentences in generated reports. To this end, we investigate how the selection of information from pathology reports for vision-language modeling affects the quality of the multimodal representations and generated reports. More concretely, we compare a model trained on full reports against a model trained on preprocessed reports that only include sentences describing the cell and tissue appearances based on the H&E-stained slides. For the experiments, we built upon the BLIP-2 framework and used a cutaneous melanocytic lesion dataset of 42,433 H&E-stained whole slide images and 19,636 corresponding pathology reports. Model performance was assessed using image-to-text and text-to-image  retrieval, as well as qualitative evaluation of the generated reports by an expert pathologist. Our results demonstrate that text preprocessing prevents hallucination in report generation. Despite the improvement in the quality of the generated reports, training the vision-language model on full reports showed better cross-modal retrieval performance.  }
}

@InProceedings{kumari25,
  title = 	 {Attention-based Generative Latent Replay: A Continual Learning Approach for WSI Analysis},
  author =       {Kumari, Pratibha and Reisenb\"{u}chler, Daniel and Bozorgpour, Afshin and Schaadt, Nadine S. and Feuerhake, Friedrich and Merhof, Dorit},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {12-23},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {Whole slide image (WSI) classification has emerged as a powerful tool in computational pathology, but remains constrained by domain shifts, e.g., due to different organs, diseases, or institution-specific variations. To address this challenge, we propose an Attention-based Generative Latent Replay Continual Learning framework (AGLR-CL), in a multiple instance learning (MIL) setup for domain incremental WSI classification. Our method employs Gaussian Mixture Models (GMMs) to synthesize WSI representations and patch count distributions, preserving knowledge of past domains without explicitly storing original data. A novel attention-based filtering step focuses on the most salient patch embeddings, ensuring high-quality synthetic samples. This privacy-aware strategy obviates the need for replay buffers and outperforms other buffer-free counterparts while matching the performance of buffer-based solutions. We validate AGLR-CL on clinically relevant biomarker detection and molecular status prediction across multiple public datasets with diverse centers, organs, and patient cohorts. Experimental results confirm its ability to retain prior knowledge and adapt to new domains, offering an effective, privacy-preserving avenue for
domain incremental continual learning in WSI classification.}
}

@InProceedings{amer25,
  title = 	 {Scalable and Loosely-Coupled Multimodal Deep Learning for Breast Cancer Subtyping},
  author =       {Amer, Mohammed and Suliman, Mohamed A. and Bui, Tu and Garcia, Nuria and Georgescu, Serban},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {24-38},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {Healthcare applications are inherently multimodal, benefiting greatly from the integration of diverse data sources. However, the modalities available in clinical settings can vary across different locations and patients. A key area that stands to gain from multimodal integration is breast cancer molecular subtyping, an important clinical task that can facilitate personalized treatment and improve patient prognosis. In this work, we propose a scalable and loosely-coupled multimodal framework that seamlessly integrates data from various modalities, including copy number variation (CNV), clinical records, and histopathology images, to enhance breast cancer subtyping. While our primary focus is on breast cancer, our framework is designed to easily accommodate additional modalities, offering the flexibility to scale up or down with minimal overhead without requiring re-training of existing modalities, making it applicable to other types of cancers as well. We introduce a dual-based representation for whole slide images (WSIs), combining traditional imagebased and graph-based WSI representations. This novel dual approach results in significant performance improvements. Moreover, we present a new multimodal fusion strategy, demonstrating its ability to enhance performance across a range of multimodal conditions. Our comprehensive results show that integrating our dual-based WSI representation with CNV and clinical health records, along with our pipeline and fusion strategy, outperforms state-of-the-art methods in breast cancer subtyping.}
}


@InProceedings{schmidt-santiago25,
  title = 	 {Uncertainty-Aware Ensemble Segmentation of Breast Cancer Tissue Microarrays},
  author =       {Schmidt-Santiago, Lucia and Kinakh, Roman and Carreras-Salinas, Sergio and Guerrero-Aspizcua, Sara and Ríos-Muñoz, Gonzalo R. and Mu\~{n}oz-Barrutia, Arrate},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {39-51},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {Breast cancer Tissue Microarrays (TMAs) offer a high-throughput platform for studying tumor morphology and biomarker expression. We present an automated deep learning pipeline for semantic segmentation of Hematoxylin and Eosin (H&E)-stained breast cancer TMAs, integrating ensemble U-Net architectures with ResNet encoders and Monte Carlo Dropout (MCDO) for uncertainty estimation. A robust pre-processing workflow addresses illumination artifacts, staining variability, and tissue detection. Multiple U-Net models were trained using distinct loss functions to address class imbalance and feature  iversity. Predictions were combined via soft voting, emulating consensus among pathologists. Uncertainty was quantified using MCDO across ensemble outputs,
enhancing reliability and interpretability. Our pipeline outperforms similar methods such as WeGleNet (mIoU = 0.4368) and HistoSegNet (mIoU = 0.5505), achieving a mean IoU of 0.58 ± 0.11 and Dice Score of 0.66 ± 0.10. Calibration analysis shows superior alignment of standard deviation–based uncertainty estimates with actual prediction errors (UCE = 0.085 ± 0.033). This pipeline effectively segments complex histopathological structures and flags ambiguous regions for review, supporting downstream biomarker discovery and clinical interpretation.}
}


@InProceedings{wood25,
  title = 	 {GenST: A Generative Cross-Modal Model for Predicting Spatial Transcriptomics from Histology Images},
  author =       {Wood, Ruby and Hu, Yang and Rittscher, Jens and Li, Bin},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {52-65},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {Spatial transcriptomics is used to identify gene expression levels in certain locations across a tissue sample, preserving important spatial information in cancerous tissue samples for  downstream clinical decision making. However, this technology is currently too expensive to be used in a routine clinical pathways. On the other hand, digital images of haematoxylin and eosin stained histology slides are routinely generated from tissue biopsy samples. Here, we develop a generative cross-modal method to predict spatial transcriptomics from histology images by aligning the latent space of two VQ-VAEs for each modality. We benchmark our approach on multiple sequencing technologies (Visium and ST) and cancer types (breast, brain, spinal cord and skin) from two public datasets, using 142 slides with 820,407 spots from STImage-1K4M (Chen et al., 2024a) and 568 slides with 254,812 spots from HEST-1k (Jaume et al., 2024). Across the resulting cohorts, our model achieves superior performance to state-of-the-art models in half, whilst providing an interpretable framework for understanding which genetic expressions of a cancer tumour can be captured from the morphology observed in corresponding locations of the histology image.}
}

@InProceedings{keshvarikhojasteh25,
  title = 	 {A Spatially-Aware Multiple Instance Learning Framework for Digital Pathology},
  author =       {Keshvarikhojasteh, Hassan and Tifrea, Mihail and Hess, Sibylle and Pluim, Josien P.W. and Veta, Mitko},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {66-74},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {Multiple instance learning (MIL) is a promising approach for weakly supervised classification in pathology using whole slide images (WSIs). However, conventional MIL methods such as Attention-Based Deep Multiple Instance Learning (ABMIL) typically disregard spatial interactions among patches that are crucial to pathological diagnosis. Recent advancements, such as Transformer based MIL (TransMIL), have incorporated spatial context and inter-patch relationships. However, it remains unclear whether explicitly modeling patch relationships yields similar performance gains in ABMIL, which relies solely on Multi-Layer Perceptrons (MLPs). In contrast, TransMIL employs Transformer-based layers, introducing a fundamental architectural shift at the cost of substantially increased computational complexity. In this work, we enhance the ABMIL framework by integrating interaction-aware representations to address this question. Our proposed model, Global ABMIL (GABMIL), explicitly captures inter-instance dependencies while preserving computational efficiency. Experimental results on two publicly available datasets for tumor subtyping in breast and lung cancers demonstrate that GABMIL achieves up to a 7 percentage point improvement in AUPRC and a 5 percentage point increase in the Kappa score over ABMIL, with minimal or no additional computational overhead. These findings underscore the importance of incorporating patch interactions within MIL frameworks. Our code is available at GABMIL.}
}


@InProceedings{brattoli25,
  title = 	 {Identifying actionable driver mutations in lung cancer using an efficient Asymmetric Transformer Decoder},
  author =       {Brattoli, Biagio and Shi, Jack and Park, Jongchan and Lee, Taebum and Yoo, Donggeun and Pereira, Sergio},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {75-85},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {Identifying actionable driver mutations in non-small cell lung cancer (NSCLC) can impact treatment decisions and significantly improve patient outcomes. Despite guideline recommendations, broader adoption of genetic testing remains challenging due to limited availability and lengthy  urnaround times. Machine Learning (ML) methods for Computational Pathology (CPath) offer a potential solution; however, research often focuses on only one or two common mutations, limiting the clinical value of these tools and the pool of patients who can benefit from them. This study evaluates various Multiple Instance Learning (MIL) techniques to detect six key actionable NSCLC driver mutations: ALK, BRAF, EGFR, ERBB2, KRAS, and MET ex14. Additionally, we introduce an Asymmetric Transformer Decoder model that employs queries and key-values of varying dimensions to maintain a low query dimensionality. This approach efficiently extracts information from patch embeddings and minimizes overfitting risks, proving highly adaptable to the MIL setting. Moreover, we present a method to directly utilize tissue type in the model, addressing a typical MIL limitation where either all regions or only some specific regions are analyzed, neglecting biological relevance. Our method outperforms top MIL models by an average of 3%, and over 4% when predicting rare mutations such as ERBB2 and BRAF, moving ML-based tests closer to being practical alternatives to standard genetic testing.}
}

@InProceedings{richter25,
  title = 	 {Linear Attention-based Multiple Instance Learning for Computational Pathology},
  author =       {Richter, Charlotte and Reisenb\"{u}chler, Daniel and Schaadt, Nadine S. and Feuerhake, Friedrich and Merhof, Dorit},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {86-96},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {Deep learning–based analysis of gigapixel whole slide images (WSIs) in computational pathology (CPath) typically relies on patch-level feature extraction and instance aggregation, with attention-based contextualization at the core of state-of-the-art methods. However, scalability is a major challenge due to the vast number of patches. Therefore, we introduce linear attention based multiple-instance learning (Lin-MIL), which transposes and interchanges the calculations of queries, keys, and values in the attention mechanism. By leveraging linear attention, Lin-MIL reduces computational complexity from O(n^2d) to O(nd^2), compared to vanilla self-attention. Despite this efficiency gain, LinMIL outperforms 12 baseline methods across biomarker, mutation,  and tumor classification benchmarks, while also demonstrating robust out-of-domain performance. Moreover, its qualitative attention maps highlight  diagnostically relevant regions. In summary, Lin-MIL provides increased performance as well as enhanced scalability and interpretability for a range of computational pathology tasks. Code available at https://github.com/charlotterchtr/Lin-MIL.}
}

@InProceedings{hensens25,
  title = 	 {ContriMix: Scalable stain color augmentation for domain generalization without domain labels in digital pathology},
  author =       {Hensens, Lisa and Sabroso-Lasa, Sergio and Verbeke, Caroline and Malats, Nuria and consortium, ThePanGenEU and Litjens, Geert and Vendittelli, Pierpaolo},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {97-105},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {Pancreatic ductal adenocarcinoma (PDAC) remains one of the deadliest cancers due to late detection and limited treatment response. This study investigates the prognostic value of combining multiple AI-based image biomarkers—tumor-stroma ratio (TSR), mitosis density, stromal cell density (via HoVer-Net), tumor-to-tissue ratio from histopathological whole-slide images (WSIs) for survival prediction in resected PDAC patients. A multi-tissue segmentation model was developed to generate tissue masks for downstream biomarker extraction. Using logistic and Cox regression models, both univariate and multivariate survival analyses were performed across four datasets. Results show that while combining biomarkers did not outperform single-biomarker models (notably TSR), mitosis density showed consistent statistical significance and may serve as a valuable prognostic feature.}
}


@InProceedings{rashid25,
  title = 	 {KidneyGrader: Fine-Grained Tubulitis Scoring Using Weakly Supervised Transformers},
  author =       {Rashid, Abrar and Jain, Vishal and Cechnicka, Sarah and Chaudry, Aamir and Roufosse, Candice and Kainz, Bernhard},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {106-115},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {Accurate tubulitis scoring is essential for managing kidney transplant rejection, yet manual assessment is subjective and suffers from severe inter-rater variability (κw=0.17), leading to inconsistent treatment decisions. While recent works have attempted binary tubulitis detection, fine-grained scoring (T0-T3) required for clinical decision-making remains unaddressed. We present the first automated approach for granular tubulitis scoring using only slide-level supervision. Our approach aggregates spatially correlated features from tubulecentric image patches using a transformer-based attention pooling mechanism. To ensure diagnostic focus, patches are pre-filtered using a segmentation model trained to detect renal tubules, restricting the input space to regions most relevant for scoring. Evaluated on 93 routine PAS-stained slides (75 for training/validation, 18 held-out test), our method achieves a weighted kappa of κw = 0.75 (4.4× improvement over expert agreement), 83.3% within-one-grade accuracy, and strong correlation with expert scores (r = 0.81). Topattended regions demonstrate clinical plausibility, showing progressively greater inflammatory burden and tissue damage features with increasing T-scores. Our work demonstrates that weakly supervised learning can transform subjective pathology assessments into reliable, interpretable predictions, offering a practical path towards standardising transplant rejection diagnosis. The code is available on github.}
}

@InProceedings{cho25,
  title = 	 {MVHybrid: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models},
  author =       {Cho, Won June and Yoon, Hongjun and Jeong, Daeky and Lim, Hyeongyeol and Chong, Yosep},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {116-138},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {Spatial transcriptomics reveals gene expression patterns within tissue context, enabling precision oncology applications such as treatment response prediction, but its high cost and technical complexity limit clinical adoption. Predicting spatial gene expression (biomarkers) from routine histopathology images offers a practical alternative, yet current vision foundation models (VFMs) in pathology based on Vision Transformer (ViT) backbones 
perform below clinical standards. Given that VFMs are already trained on millions of diverse whole slide images, we hypothesize that architectural innovations beyond ViTs may better capture the low-frequency, subtle morphological patterns correlating with molecular phenotypes. By demonstrating that state space models initialized with negative real eigenvalues exhibit strong low-frequency bias, we introduce MVHybrid, a hybrid backbone architecture combining state space models (SSMs) with ViT. We compare five other different backbone architectures for pathology VFMs, all pretrained on identical colorectal cancer datasets using the DINOv2 self-supervised learning method. We evaluate all pretrained models using both random split and leave-one-study-out (LOSO) settings of the
same biomarker dataset. In LOSO evaluation, MVHybrid achieves 57% higher correlation than the best-performing ViT and shows 43% smaller performance degradation compared to random split in gene expression prediction, demonstrating superior performance and robustness, respectively. Furthermore, MVHybrid shows equal or better downstream performance in classification, patch retrieval, and survival prediction tasks compared to that of ViT, showing its promise as a next-generation pathology VFM backbone. Our code is publicly available at: https://github.com/deepnoid-ai/MVHybrid.}
}

@InProceedings{li25,
  title = 	 {Enhancing Interpretation of Histopathology Whole Slide Image Analysis via Regional Causal Dependency Discovery},
  author =       {Li, Zixian and Shi, Jun and Jiang, Zhiguo and Xie, Fengying and Zheng, Yushan},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {139-149},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {Histopathology whole slide image (WSI) analysis is fundamental to computational pathology. Attention-based heatmaps are commonly used for interpretability in WSI analysis. However, heatmap is limited in describing the potential relationships between  multiple high-probability regions, which restricts its application in fine-grained WSI analysis tasks. In this paper, we propose Pathology Causal Discovery Network (PCDN), a novel framework that reconstructs interpretable diagnostic pathways by dynamically discovering regional causal dependencies from WSIs. Unlike approaches relying on predefined medical priors, PCDN introduces a Causal Structure Learner (CSL) to infer a Directed Acyclic
Graph (DAG) which represents the causal dependencies among pathological regions. A Causal Graph Propagator (CGP) is then designed to guide feature propagation based on the DAG, integrating local causal dependencies with global context. Extensive experiments on three large-scale pathological datasets demonstrate that PCDN achieves state-of-the-art performance and can provide meaningful causal insights for WSI analysis.}
}

@InProceedings{syazwany25,
  title = 	 {Centroid-Aware Gaussian Prompt Learning with Erosion-Guided Accumulator for Robust Semantic Cell Segmentation},
  author =       {Syazwany, Nur Suriza and Kim, Su Jung and Nam, Ju-Hyeon and Lee, Sang-Chul},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {160-170},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {Accurately segmenting cell images remains challenging due to variations in cell size, shape,
and overlapping structures. Existing approaches often struggle with densely packed and overlapping cell regions, leading to inconsistent performance. While recent methods, such as the Segment Anything Model (SAM), have shown promise, they rely heavily on manual prompting, which can be time-consuming and inconsistent for densely packed nuclei datasets. To address these limitations, we propose a novel centroid-guided Gaussian mapbased accumulator prompting approach for robust nuclei segmentation. Our method constructs Gaussian maps by accumulating centroids across multiple erosion iterations, capturing the frequency and spatial distribution of nuclei centroids. These maps serve as informative priors to guide the segmentation model, enhancing its ability to localize cell structures while maintaining adaptability to varying cell sizes. By integrating these Gaussian-based prompts into a transformer-based segmentation model, our approach enables refined predictions with improved spatial awareness. We validate our method on two challenging, densely packed datasets, DSB18 and ConSeP, demonstrating robust and superior segmentation performance over state-of-the-art methods.}
}

@InProceedings{nguyen25,
  title = 	 {fmMAP: A Framework Reducing Site-Bias Batch Effect from Foundation Models in Pathology},
  author =       {Nguyen, Hai Cao Truong and Ho, David Joon},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {171-186},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {Foundation models (FMs) in pathology are general-purpose models capturing heterogeneous morphological patterns on pathology images leveraged by a vast training dataset.Although FMs have demonstrated promising results in multiple downstream tasks such as classification and retrieval, confounding factors are also embedded in the features potentially causing inaccurate decisions. For example, we observe a batch effect where distinctive medical center signatures are displayed when clustering features from FMs. In this work, we propose Foundation Model-based Manifold Approximation Pipeline (fmMAP) to reduce the batch effect by adjusting features from FMs. Our framework employs supervised uniform manifold approximation (UMAP) to transform features generated by FMs into an optimal space. In this transformed space, characteristics of features of interest (i.e., biological features) are highlighted while other confounding factors are reduced. Experimental results on eight recent FMs show that raw features from the FMs are shown to be unrobust, but fmMAP transforms features to become robust on all FMs according to the robustness index. In addition, fmMAP reduces average balanced accuracy for site prediction and improves average balanced accuracy for tissue type classification achieving more than 96% in publicly available datasets. We expect fmMAP framework will help FMs identify essential pathologic features that would enhance performance on downstream tasks. The code is available at https://github.com/davidholab/fmMAP.}
}

@InProceedings{corso25,
  title = 	 {Context-guided Prompt Learning for Continual WSI Classification},
  author =       {Corso, Giulia and Miccolis, Francesca and Porrello, Angelo and Bolelli, Federico and Calderara, Simone and Ficarra, Elisa},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {187-198},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {Whole Slide Images (WSIs) are crucial in histological diagnostics, providing high-resolution insights into cellular structures. In addition to challenges like the gigapixel scale of WSIs and the lack of pixel-level annotations, privacy restrictions further complicate their analysis. For instance, in a hospital network, different facilities need to collaborate on WSI analysis without the possibility of sharing sensitive patient data. A more practical and secure approach involves sharing models capable of continual adaptation to new data. However, without proper measures, catastrophic forgetting can occur. Traditional continual learning techniques rely on storing previous data, which violates privacy restrictions. To address this issue, this paper introduces Context Optimization Multiple Instance Learning (CooMIL), a rehearsal-free continual learning framework explicitly designed for WSI analysis. It employs a WSI-specific prompt learning procedure to adapt classification models
across tasks, efficiently preventing catastrophic forgetting. Evaluated on four public WSI datasets from TCGA projects, our model significantly outperforms state-of-the-art methods within the WSI-based continual learning framework. The source code is available at https://github.com/FrancescaMiccolis/CooMIL.}
}

@InProceedings{vitoria25,
  title = 	 {A Benchmark of Foundation Model Encoders for Histopathological Image Segmentation},
  author =       {Vitoria, Itsaso and Saratxaga, Cristina L. and Lago, Cristina Penas and Izu, Rosa and Sanchez-Diez, Ana and Cancho-Galan, Goikoana and Boyano, Maria Dolores and Arganda-Carreras, Ignacio and Galdran, Adrian},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {199-212},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {Whole-slide imaging has transformed histopathology into a data-intensive field, requiring robust and generalisable computational tools. Foundation models offer a promising approach for a range of downstream tasks with minimal labelled data. While recent work has shown their effectiveness for slide-level classification and retrieval, their potential for dense prediction tasks such as image segmentation remains underexplored. In this study, we present a comprehensive benchmark of 15 pathology-specific foundation models for histopathological image segmentation, evaluated across two distinct modalities: H&Estained histology and Annexin A5-stained immunohistochemistry. To ensure a fair and architecture-neutral comparison, we freeze each foundation models encoder and pair it with a shared lightweight decoder, disentangling representation quality from model size. Results show that foundation model encoders can sometimes lead to strong segmentation performance without fine-tuning, but effectiveness varies significantly by model and modality. Our findings reveal that compact encoders can often outperform larger, more recent models, underscoring that model size and classification accuracy are poor predictors of segmentation capabilities.}
}


@InProceedings{brandstatter25,
  title = 	 {Semantic Mosaicing of Histo-Pathology Image Fragments using Visual Foundation Models},
  author =       {Brandst\"{a}tter, Stefan and K\"{o}ller, Maximilan and Seeb\"{o}ck, Philipp and Blessing, Alissa and Oberndorfer, Felicitas and Pochepnia, Svitlana and Prosch, Helmut and Langs, Georg},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {213-222},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {In histopathology, tissue samples are often larger than a standard microscope slide, making stitching of multiple fragments necessary to process entire structures such as tumors.Automated stitching is a prerequisite for scaling analysis, but is challenging due to possible tissue loss during preparation, inhomogeneous morphological distortion, staining inconsistencies, missing regions due to misalignment on the slide, or frayed tissue edges. This limits state-of-the-art stitching methods using boundary shape matching algorithms to reconstruct artificial whole mount slides (WMS). Here, we introduce SemanticStitcher using latent feature representations derived from a visual histopathology foundation model to identify neighboring areas in different fragments. Robust pose estimation based on a large number of semantic matching candidates derives a mosaic of multiple fragments to form the WMS. Experiments on three different histopathology datasets demonstrate that SemanticStitcher yields robust WMS mosaicing and consistently outperforms the state of the art in correct boundary matches.}
}


@InProceedings{helgesen25,
  title = 	 {Reliable and Efficient Tissue Segmentation in Whole-Slide Images},
  author =       {Helgesen, Sander Elias Magnussen and Manet, Anthony and Cyll, Karolina and Tobin, Kari Anne Risan and Kjæreng, Marna Lill and Kostolomov, Ily´a and Henriksen, Audun Ljone and Raedt, Sepp de and Askautrud, Hanne Arenberg and Lacle, Miangela and Jones, Robert and Verhoef, Cornelis and Hveem, Tarjei Sveinsgjerd and Skrede, Ole-Johan and Kleppe, Andreas },
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {223-233},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {Whole-slide images in digital pathology often contain large regions of irrelevant background, making tissue segmentation an important preprocessing step in many applications. Traditional rule-based approaches to tissue segmentation often work quite well, but it is difficult to create general rules that cover all instances. We here apply an unmodified nnU-Net v2 training setup on downsampled whole-slide to develop and test an efficient and robust tissue segmentation model. The dataset contained nearly 30 000 images from slides with different tissue types, imaged using different scanners, and annotated using a semiautomatic workflow so that all annotations have been verified or made by human experts. This large, diverse dataset enables the training of a tissue segmentation model that generalizes well across different scanners and tissue types. We observed that our proposed model achieves similar or better accuracy than other deep learning models, while offering better robustness than simpler rule-based methods. The best compromise between inference speed and accuracy was observed using images at 10 µm per pixel. Our approach can be
used as an efficient and well-suited preprocessing step for computational pathology. Source code, Dockerfiles, and model weights are made publicly available at: https://github.com/icgi/Reliable-and-Efficient-Tissue-Segmentation-in-Whole-Slide-Images.}
}


@InProceedings{bøe25,
  title = 	 {Low-Rank Adaptations for increased Generalization in Foundation Model features},
  author =       {Bøe, Vilde Schulerud and Kleppe, Andreas and Foersch, Sebastian and Wagner, Daniel-Christoph and Busund, Lill-Tove Rasmussen and Rivera, Adín Ramírez},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {234-247},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {For foundation models (FMs) to truly advance computational pathology, they must deliver consistent and reliable predictions under diverse, unseen test conditions. Without such robustness, clinical trust and widespread adoption remain out of reach. Although many FMs for histopathology now exist, they have to our knowledge not been systematically tested for robustness by external researchers on independent datasets. In this study, we evaluate the robustness of foundation model features on three separate  histopathology datasets and find that their performance drops on external data. Our analysis also reveals that these models often encode dataset-specific information, limiting their generalizability. To address this issue, we train a Weight-Decomposed Low-Rank Adaptation (DoRA) with strong data augmentations to improve feature robustness. Our experiments show that models trained with this adapter exhibit fewer signs of dataset-specific information and may generate more robust features across domains. These results highlight the need for robustness testing and encourage incorporating robustness considerations into the development, training, and tuning of FMs for histopathology. The code for this work will be available at https://github.com/dsb-ifi/DoRA-for-FM-robustness.}
}

@InProceedings{ram25,
  title = 	 {Towards Automated Banff Lesion Scoring: Tissue Segmentation in Kidney Transplant Biopsies using Deep Learning},
  author =       {Ram, Sebastiaan and Midden, Dominique van and Laak, Jeroen van der and Studer, Linda},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {248-265},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {Inflammation and chronic changes in the different tissue structures (e.g., glomeruli, tubuli, interstitium) are major contributors to kidney transplant failure. Kidney transplant biopsy diagnostics is based on the Banff classification system, in which pathologists assess these changes. However, many of these factors have suboptimal reproducibility and the scoring is labor-intensive. To address this, we developed a multi-class segmentation approach that covers all tissue structures relevant for diagnostics. Our dataset comprises 99 Periodic-acid Schiff (PAS)-stained kidney transplant biopsy slides from two pathology departments. An expert pathologist manually annotated >17,000 structures across eight classes (glomeruli, sclerotic glomeruli, empty Bowman space, proximal tubuli, distal tubuli, atrophic tubuli, capsule, arteries/arterioles, and interstitium). We compared two segmentation approaches: (1) a combination of two nnU-Nets (one for tissue segmentation and one specialized for structure boundary detection) and (2) the SAM-Path foundation model. For the peritubular capillary segmentation, we used a previously developed U-Net. The nnU-Nets achieved a per-class average Dice score of 0.80, outperforming SAM-Path (0.69) and providing a reliable solution for all tissue structures relevant for kidney transplant biopsy  diagnostics. Next, the nnU-Nets will be used in a reader study aimed at investigating the impact of AI on pathologists’ performance in Banff lesion scoring. The algorithm is publicly available on Grand Challenge.}
}

@InProceedings{banerjee25,
  title = 	 {Chromosome Mask-Conditioned Generative Inpainting for Atypical Mitosis Classification},
  author =       {Banerjee, Sweta and Weiss, Viktoria and Conrad, Thomas and Donovan, Taryn A. and Ammeling, Jonas and Fick, Rutger H.J. and Utz, Jonas and Klopfleisch, Robert and Kaltenecker, Christopher and Bertram, Christof A. and Breininger, Katharina and Aubreville, Marc},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {266-277},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {Atypical mitoses are critical prognostic markers for tumor proliferation, yet classification efforts are compromised by class imbalance, data scarcity, and noisy labels. Our work focuses on hematoxylin and eosin (H&E)-stained histopathology images, where identifying such mitoses is particularly challenging due to overlapping morphological features and stain variability. We address these challenges with a novel approach for biologically informed inpainting, conditioned on a histological context patch, an inpainting mask, and a chromosome segmentation mask. This triple-conditioned generative strategy allows disentanglement of the mitotic figure  shape information from the cellular context and enables the utilization of large-scale datasets that do not contain atypical sub-classification for training classification 
models. We evaluate both adversarial and denoising diffusion-based inpainting strategies.Our approach mitigates the lack of data diversity and label noise, thereby substantially improving classification performance for atypical vs. normal mitoses - as demonstrated by downstream classification with EfficientNet-B0 and Low-rank  adaptation (LoRA) finetuned foundation models. We provide the complete source code, including all our methods, at our github repository: https://github.com/DeepMicroscopy/ChroMa-GI.}
}

@InProceedings{pham25,
  title = 	 {Learning Disentangled Stain and Structural Representations for Semi-Supervised Histopathology Segmentation},
  author =       {Pham, Ha-Hieu and Vu, Nguyen Lan Vi and Nguyen, Thanh-Huy and Bagci, Ulas and Xu, Min and Le, Trung-Nghia and Pham, Huy-Hieu},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {278-287},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {Accurate gland segmentation in histopathology images is essential for cancer diagnosis and prognosis. However, significant variability in Hematoxylin and Eosin (H&E) staining and tissue morphology, combined with limited annotated data, poses major challenges for automated segmentation. To address this, we propose Color-Structure Dual-Student (CSDS), a novel semi-supervised segmentation framework designed to learn disentangled representations of stain appearance and tissue structure. CSDS comprises two specialized student networks: one trained on stain-augmented inputs to model chromatic variation, and the other on structure-augmented inputs to capture morphological cues. A shared teacher network, updated via Exponential Moving Average (EMA), supervises both students through pseudo-labels. To further improve label reliability, we introduce stain-aware and structureaware uncertainty estimation modules that adaptively modulate the contribution of each student during training. Experiments on the GlaS and CRAG datasets show that CSDS achieves state-of-the-art performance in low-label settings, with Dice score improvements of up to 1.2% on GlaS and 0.7% on CRAG at 5% labeled data, and 0.7% and 1.4% at 10%. Our code and pre-trained models are available at https://github.com/hieuphamha19/CSDS.}
}

@InProceedings{qiu25,
  title = 	 {Effortless Vision-Language Model Specialization in Histopathology without Annotation},
  author =       {Qiu, Jingna and Jain, Nishanth and Ammeling, Jonas and Aubreville, Marc and Breininger, Katharina},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {288-300},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {Recent advances in Vision-Language Models (VLMs) in histopathology, such as CONCH and QuiltNet, have demonstrated impressive zero-shot classification capabilities across various tasks. However, their general-purpose design may lead to suboptimal performance in specific downstream applications. While supervised fine-tuning methods address this issue, they require manually labeled samples for adaptation. This paper investigates annotationfree adaptation of VLMs through continued pretraining on domain- and task-relevant image-caption pairs extracted from existing databases. Our experiments on two VLMs, CONCH and QuiltNet, across three downstream tasks reveal that these pairs substantially enhance both zero-shot and few-shot performance. Notably, with larger training sizes, continued pretraining matches the performance of few-shot methods while eliminating manual labeling. Its effectiveness, task-agnostic design, and annotation-free workflow make it a promising pathway for adapting VLMs to new histopathology tasks. Code is available at https://github.com/DeepMicroscopy/Annotation-free-VLM-specialization.}
}

@InProceedings{lohmann25,
  title = 	 {From Gene Expression to Tissue Morphology: Can Generative Models Uncover the Link?},
  author =       {Lohmann, Frederieke and Valdeolivas, Alberto and Vasiljevic, Jelica},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {301-317},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {Spatial Transcriptomics technologies enable capturing gene expression within the native tissue context. Platforms such as 10x Visium and Visium HD integrate gene expression with histological imaging, providing a multi-dimensional view of tissue organisation. Motivated by the success of generative models in computer vision and natural language processing, we investigate the largely unexplored task of synthesising histological images directly from gene expression profiles. Leveraging recent advancements in Spatial Transcriptomics, particularly the 10X Visium HD platform,
we introduce the first two-stage conditional generative framework to infer tissue morphology from near-whole transcriptome profiles. Competitive FID scores and a study involving multiple pathologists confirm that the synthesised images are plausible and that our framework generalises well to unseen standard Visium samples. Furthermore, model interpretation reveals connections between structurally relevant gene sets and specific morphological patterns, opening new avenues for studying the relationship between gene expression and tissue morphology.}
}

@InProceedings{miccolis25,
  title = 	 {OXA-MISS: A Robust Multimodal Architecture for Chemotherapy Response Prediction under Data Scarcity},
  author =       {Miccolis, Francesca and Marinelli, Fabio and Pipoli, Vittorio and Afenteva, Daria and Virtanen, Anni and Lovino, Marta and Ficarra, Elisa},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {318-327},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {In clinical oncology, tumor heterogeneity, data scarcity, and missing modalities are pervasive issues that significantly hinder the effectiveness of predictive models. Although multimodal integration of Whole Slide Imaging (WSI) and molecular data has shown promise in predicting overall survival (OS), current approaches often struggle when dealing with scarce and incomplete multimodal datasets, a scenario that reflects the norm rather than the exception in real-world clinical practice, especially in tasks like chemotherapy resistance prediction, where data collection is substantially more challenging than for OS. Accurately identifying patients who will not respond to chemotherapy is a critical clinical need, enabling the timely redirection to alternative therapeutic strategies and avoiding unnecessary toxicity. Hence, this paper introduces OXA-MISS, a novel multimodal model for chemotherapy response prediction designed to handle missing modalities. In the task of chemotherapy response prediction in ovarian cancer, OXA-MISS achieves a 20% absolute improvement in AUC over state-of-the-art models when trained on scarce and incomplete WSI–transcriptomics datasets. To evaluate its generalizability, we benchmarked OXA-MISS on OS prediction across three TCGA cancer types under both complete and missing-modality conditions. In these settings, the results demonstrate that OXA-MISS achieves performance comparable to that of state-of-the-art models. In conclusion, the proposed OXA-MISS is shown to be effective in OS prediction tasks, while substantially improving predictive accuracy in realistic clinical settings, such as the proposed prediction of chemotherapy response. The code for OXA-MISS is publicly available at https://github.com/AI-BioInformatics/OXA-MISS.}
}


@InProceedings{gutwein25,
  title = 	 {NeXtMarker: Contrastive Learning for Marker-Level Interpretability in Single-Cell Multiplex Imaging},
  author =       {Gutwein, Simon and Lazic, Daria and Walter, Thomas and Taschner-Mandl, Sabine and Licandro, Roxane},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {328-337},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {Understanding cell phenotypes and their spatial organization is crucial in multiplex imaging for spatial biology. Conventional analysis pipelines rely on extensive preprocessing, including background correction and segmentation, introducing biases and information loss. We
present NeXtMarker, an interpretable deep learning framework for end-to-end single-cell analysis of multiplex images, eliminating the need for manual preprocessing or segmentation. NeXtMarker employs learned marker-specific normalization and interpretable feature extraction to generate biologically meaningful embeddings in a fully self-supervised manner. It directly processes raw images of cells while preserving spatial and morphological information. We demonstrate NeXtMarker’s ability to (i) resolve intercellular expression patterns and cell morphology, (ii) enable accurate cell phenotyping in a large neuroblastoma tumor dataset, and (iii) generalize to independent osteosarcoma images. NeXtMarker maintains high agreement with conventional pipelines while eliminating the need for preprocessing and segmentation and enhancing interpretability. By enabling unbiased, scalable singlecell analysis, NeXtMarker establishes a foundation for improved phenotyping in multiplex imaging. Code and pretrained models available at: [code_released_upon_acceptance].}
}


@InProceedings{cui25,
  title = 	 {WSI-BayesUNet: Uncertainty-Aware Deep Learning for Histopathological Image Segmentation with Active Learning},
  author =       {Cui, Yijun and Litjens, Geert and Nadieh, Khalili},
  booktitle = 	 {Proceedings of the MICCAI Workshop on Computational Pathology},
  pages = 	 {338-346},
  year = 	 {2025},
  month = 	 {27 Sep},
  editor =    {Studer, Linda and Ciompi, Francesco and Khalili, Nadieh and Faryna, Khrystyna and Faryna, Khrystyna and Yeong, Joe and Lau, Mai Chan and Chen, Hao and Liu, Ziyi and Brattoli, Biagio},
  volume = 	 {316},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {Histopathological image segmentation is a core task in digital pathology, supporting applications such as cancer detection and subtype classification. Manual annotation is time-consuming and subjective, making automation essential for improving efficiency and consistency in diagnostic workflows. Although deep learning models have significantly automated this process, they still make silent mistakes. Quantifying the uncertainty of the model and using the uncertainty for further improvement is not fully addressed. The most common way to quantify uncertainty is through ensemble methods, which provide empirical uncertainty estimation but face limitations, including high computational costs and theoretical instability. To address these, we propose a Bayesian U-Net framework that employs variational inference for principled probabilistic uncertainty estimation. Leveraging active learning, our Bayesian U-Net iteratively improves segmentation performance by prioritizing the most uncertain samples. Experiments on the TIGER and CAMELYON17 datasets show that Bayesian U-Net outperforms ensemble methods, offering better uncertainty quantification, uncertainty-guided performance gains, and faster convergence. Notably, uncertainty-based sampling consistently surpasses random sampling, significantly reducing annotation effort while maintaining or improving segmentation accuracy.}
}